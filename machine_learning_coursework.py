{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# %% [markdown]\n# <span style=\"font-size:30px;\"> **Machine Learning Coursework - Hassaan Naveed** </span>\n\n# %% [markdown]\n# <span style=\"font-size:25px;\"> **1. Preparation** </span>\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **1.1. Importing Modules** </span>\n\n# %% [code]\n# Imports\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom yellowbrick.cluster import KElbowVisualizer\n\nimport plotly.graph_objects as go\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **1.2. Reading the dataset and displaying its info** </span>\n\n# %% [code]\n# Read data and output information\nsurvey = pd.read_csv(\"../input/stackoverflow-survey-2020/survey_results_public.csv\")\nsurvey.info()\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **1.3. Selecting features** </span>\n\n# %% [code]\n# Select features. 'ConvertedComp' is the target variable: salary\ncolumns = ['ConvertedComp',\n           'YearsCodePro',\n           'Country',\n           'EdLevel', \n           'DevType',\n           'LanguageWorkedWith']\ndf = pd.DataFrame(survey, columns=columns)\ndf.head(10)\n\n# %% [markdown]\n# <span style=\"font-size:25px;\"> **2. Data Cleansing** </span>\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **2.1. Identifying Missing Values** </span>\n\n# %% [code]\n# Calculate total number of missing values for each feature\ndf.isna().sum()\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **2.2. Dropping rows where salary is not answered** </span>\n\n# %% [code]\n# Drop all rows containing missing values for ConvertedComp feature\ndf.dropna(subset=['ConvertedComp'], inplace=True)\ndf.head(10)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **2.3. Replace remaining missing values with their columns' modal value** </span>\n\n# %% [code]\n# Replace all remaining missing values with the modal value of that feature\nfor i in columns:\n    df[i].fillna(df[i].mode()[0], inplace=True)\n    \ndf.head(10)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **2.4. Identifying Outliers in Numerical features** </span>\n\n# %% [code]\n# Identify Outliers in ConvertedComp\nplt.figure(figsize=(8, 6), dpi=80)\ndf['ConvertedComp'].plot(kind='box', fontsize=15)\n\nplt.gcf().axes[0].yaxis.get_major_formatter().set_scientific(False)\nplt.show()\n\n# %% [code]\nplt.figure(figsize=(8, 6), dpi=80)\n\ndf['ConvertedComp'].plot(kind = 'hist', grid = True, bins = 100, fontsize = 15)\nplt.gcf().axes[0].xaxis.get_major_formatter().set_scientific(False)\n\nplt.xlabel('Salary in USD')\nplt.ylabel('Frequency')\nplt.xticks(rotation=60)\nplt.show\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **2.5. Removing Outliers** </span>\n\n# %% [code]\n# Remove rows between a min and max value for ConvertedComp\ndf = df[df['ConvertedComp'].between(2500, 250000)]\n\n# %% [markdown]\n# <span style=\"font-size:25px;\"> **3. Exploratory Data Analysis** </span>\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **3.1. Exploring ConvertedComp** </span>\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **3.1.1. Descriptive Statistics** </span>\n\n# %% [code]\n# Describing 'ConvertedComp' feature\ndf['ConvertedComp'].describe()\n\n# %% [markdown]\n# <span style=\"font-size:18px;\"> **3.1.2. Plotting Histogram to identify frequency** </span>\n\n# %% [code]\n# Exploring histogram of salaries\nplt.figure(figsize=(8, 6), dpi=80)\n\ndf['ConvertedComp'].plot(kind = 'hist', grid = True, bins = 100, fontsize = 15)\nplt.gcf().axes[0].xaxis.get_major_formatter().set_scientific(False)\n\nplt.xlabel('Salary in USD')\nplt.ylabel('Frequency')\nplt.xticks(rotation=60)\nplt.show\n\n# %% [code]\nplt.figure(figsize=(8, 6), dpi=80)\nsns.distplot(df['ConvertedComp'], color='g', bins=100, hist_kws={'alpha': 0.4});\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **3.2. Exploring Years Coding Professionally** </span>\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **3.2.1. Descriptive Statistics** </span>\n\n# %% [code]\ndf['YearsCodePro'].replace({\"Less than 1 year\": \"0\", \"More than 50 years\": \"50\"}, inplace=True)\ndf['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\ndf['YearsCodePro'].describe()\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **3.2.2. Plotting Histogram to Identify Frequency** </span>\n\n# %% [code]\nplt.figure(figsize=(8, 6), dpi=80)\n\ndf['YearsCodePro'].plot(kind = 'hist', grid = True, bins = 50, fontsize = 15)\nplt.gcf().axes[0].xaxis.get_major_formatter().set_scientific(False)\n\nplt.xlabel('Years')\nplt.ylabel('Frequency')\nplt.show\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **3.2.3. Plotting Scattegram to Identify Relationship with Salary** </span>\n\n# %% [code]\nplt.figure(figsize=(8, 6), dpi=80)\n\nplt.scatter(df['ConvertedComp'], df['YearsCodePro'])\nplt.gcf().axes[0].xaxis.get_major_formatter().set_scientific(False)\nplt.ylabel(\"Years Coding Professionally\")\nplt.xlabel(\"Salary in USD\")\nplt.show()\n\n# %% [code]\ng = sns.jointplot(x = 'YearsCodePro', y = 'ConvertedComp', data=df, kind=\"kde\", height=7)\nplt.show()\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **3.3. Exploring Countries** </span>\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **3.3.1. Visualising frequency of answers from each country** </span>\n\n# %% [code]\n# Exploring Countries\ncountries = df['Country'].value_counts()\n\nfig = go.Figure(data=go.Choropleth(\n    locations = countries.index, \n    z = countries,\n    locationmode = 'country names', \n    colorscale = 'Reds',    \n))\nfig.show()\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **3.3.2. Plotting frequency of countries** </span>\n\n# %% [code]\ntopCountries = {\"United States\": \"USA\", \n                \"United Kingdom\": \"UK\", \n                \"India\": \"India\", \n                \"Germany\": \"Germany\", \n                \"Canada\": \"Canada\"}\n\ncountry = pd.DataFrame(df, columns=['Country'])\n\ncountry['Country'] = df['Country'].astype(str)\ncountry['Country'] = df['Country'].apply(lambda x: topCountries.get(x, \"Other\"))\n\nplt.figure(figsize=(8, 6), dpi=80)\ncountry['Country'].value_counts().plot(kind='pie')\n\n# %% [code]\nplt.figure(figsize=(8, 6), dpi=80)\nsns.barplot(x=country.Country, y=df.ConvertedComp)\n\n# %% [code]\nplt.figure(figsize = (10, 6))\nax = sns.boxplot(x=country['Country'], y=df['ConvertedComp'])\nplt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor=\"k\")\nplt.xticks(rotation=45)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **3.4. Exploring Education Level** </span>\n\n# %% [code]\neducation = {\"Associate degree (A.A., A.S., etc.)\": \"Associates\", \n             \"Bachelor’s degree (B.A., B.S., B.Eng., etc.)\": \"Bachelors\", \n             \"Master’s degree (M.A., M.S., M.Eng., MBA, etc.)\": \"Masters\", \n             \"Other doctoral degree (Ph.D., Ed.D., etc.)\": \"Doctorate\",\n             \"Professional degree (JD, MD, etc.)\": \"Professional\",\n             \"Primary/elementary school\": \"Primary Education\",\n             \"Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)\": \"Secondary Education\", \n             \"Some college/university study without earning a degree\": \"Some higher education, no degree\"}\n\ndf['EdLevel']  = df['EdLevel'].astype(str) #shorten description\ndf['EdLevel'] = df['EdLevel'].apply(lambda x: education.get(x, \"No formal education\"))\n\ngrouped = df.groupby('EdLevel')\n\n# %% [code]\nplt.figure(figsize=(8, 6), dpi=80)\ngrouped['EdLevel'].value_counts().plot(kind='barh')\n\n# %% [code]\ngrouped['ConvertedComp'].median().plot.barh();\n\n# %% [code]\ncrosstab = pd.crosstab(df[\"EdLevel\"], country[\"Country\"])\ncrosstab.plot(kind = 'bar', stacked = 'true',grid=True)\nplt.xlabel('Education Level')\nplt.ylabel('Frequency')\nplt.show() \n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **3.6. Exploring Developer Roles** </span>\n\n# %% [code]\n# Exploring Developer Roles compared to Salary\n\n# Split all developer types by ';' and add to set\ndevType = set()\nfor i in df['DevType']:\n    for y in i.split(';'):\n        devType.add(y.strip())\ndevType = sorted(list(devType))\n\n# Create dictionary and store all developer types as the key\ndevTypeSalaries = {}\nfor i, y in enumerate(devType):\n    devTypeSalaries[y] = i\n\n# Organise salary based on developer type\nsalaries = [[] for i in range(len(devType))]\nfor i, y in df.iterrows():\n    devlist = y['DevType'].split(';')\n    for d in devlist:\n        salaries[devTypeSalaries[d.strip()]].append(y['ConvertedComp'])\n\n# Calculate Average Salaries\naveSalary = []\nfor i in salaries:\n    aveSalary.append(np.median(i))\n    \n# Create dataframe\ndevSalary = pd.DataFrame()\ndevSalary[\"DeveloperType\"] = devType\ndevSalary[\"AverageSalary\"] = aveSalary\n\n# Plot results\nplt.subplots(figsize=(15,7))\nsns.set_style(\"whitegrid\")\nsal = sns.barplot(x=devSalary.DeveloperType, y=devSalary.AverageSalary)\nsal.set_xticklabels(devSalary.DeveloperType, rotation=90)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **3.7 Exploring Languages Worked With** </span>\n\n# %% [code]\n#Exploring most used Languages compared to Salary\n\n# Split all languages by ';' and add to set\nlanguage = set()\nfor i in df['LanguageWorkedWith']:\n    for y in i.split(';'):\n        language.add(y.strip())\nlanguage = sorted(list(language))\n\n# Create dictionary and store all languages as the key\nlangDict = {}\nfor i, y in enumerate(language):\n    langDict[y] = i\n\n# Organise salary based on language\nsalaries = [[] for i in range(len(language))]\nfor i, y in df.iterrows():\n    langList = y['LanguageWorkedWith'].split(';')\n    for l in langList:\n        salaries[langDict[l.strip()]].append(y['ConvertedComp'])\n\n# Calculate Average Salaries\naveSalery = []\nfor i in salaries:\n    aveSalery.append(np.median(i))\n\n# Create dataframe   \nlangSalary = pd.DataFrame()\nlangSalary[\"Language\"] = language\nlangSalary[\"AverageSalary\"] = aveSalery\n\n# Plot results\nplt.subplots(figsize=(15,7))\nsns.set_style(\"whitegrid\")\nsal = sns.barplot(x=langSalary.Language, y=langSalary.AverageSalary)\nsal.set_xticklabels(langSalary.Language, rotation=90)\n\n# %% [markdown]\n# <span style=\"font-size:25px;\"> **4. Data Preparation** </span>\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **4.1.Binning Numerical Features** </span>\n\n# %% [code]\ncut_labels = [\"Short\", \"Medium\", \"Long\", \"Very Long\", \"Extremely Long\"] # bin lables by texts\ncut_bins = np.linspace(min(df['YearsCodePro']), max(df['YearsCodePro']), 5) #bin range\ncut_bin = [0, 4.99, 14.99, 29.99, 39.99, 50] #bin range\ndf['Years_bin'] = pd.cut(df['YearsCodePro'], bins=cut_bin, labels=cut_labels, include_lowest=True) # map values to bins\ndf['Years_bin'] = df['Years_bin'].astype('object')\n\ndf.drop('YearsCodePro', axis=1, inplace=True)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **4.2. Encoding Ordinal Features with OrdinalEncoder** </span>\n\n# %% [code]\nencoderYr = OrdinalEncoder(categories = [[\"Short\", \"Medium\", \"Long\", \"Very Long\", \"Extremely Long\"]])\ndf['Years_bin'] = encoderYr.fit_transform(df['Years_bin'].values.reshape(-1, 1))\n\n# %% [code]\n# Encoding Categorical Values (Ordinal)\nencoderEd = OrdinalEncoder(categories = [['No formal education', \n                                           'Primary Education', \n                                           'Secondary Education', \n                                           'Some higher education, no degree',\n                                           'Associates', \n                                           'Bachelors',\n                                           'Professional',\n                                           'Masters', \n                                           'Doctorate']])\n\n\ndf['EdLevel'] = encoderEd.fit_transform(df['EdLevel'].values.reshape(-1, 1))\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **4.3. Encoding Nominal Features with OneHotEncoder** </span>\n\n# %% [code]\n%%time\n\n# Nominal Data (OneHotEncoder)\ndf = pd.get_dummies(df,prefix=['Country'], columns = ['Country'])\ndf.head(10)\n\nchoices = ['DevType', 'LanguageWorkedWith']\n\n# Go through all object columns\nfor c in choices:\n    \n    # Check if there are multiple entries in this column\n    temp = df[c].str.split(';', expand=True)\n\n    # Get all the possible values in this column\n    new_columns = pd.unique(temp.values.ravel())\n    \n    for new_c in new_columns:\n        if new_c and new_c is not np.nan:\n            \n            # Create new column for each unique column\n            idx = df[c].str.contains(new_c, regex=False).fillna(False)\n            df.loc[idx, f\"{c}_{new_c}\"] = 1\n\n    # Drop the original column\n    df.drop(c, axis=1, inplace=True)\n\ndf.fillna(0, inplace=True)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **4.4. Create Target Variable** </span>\n\n# %% [code]\nsalary_median = df['ConvertedComp'].median() #create the target variable y\ny = df['ConvertedComp'].apply(lambda x:0 if x <= salary_median else 1) #1 for high, 0 for low\ndf['Income'] = y # income is a categorical variable\ndf.drop('ConvertedComp', axis=1, inplace=True)\ndf.head(10)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **4.5. Create Feature Set** </span>\n\n# %% [code]\nX = pd.DataFrame()\nX = df.iloc[:,:-1]\n\n# %% [code]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# %% [markdown]\n# <span style=\"font-size:25px;\"> **5. Cluster Analysis** </span>\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **5.1. Clustering Models** </span>\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **5.1.1. Grouping High and Low Income** </span>\n\n# %% [code]\ngrouped = df.groupby('Income')  #group data into low and high incomes\nlowIncome = pd.DataFrame(grouped.get_group(0), columns=df.columns)  # 0 = Low income\nhighIncome = pd.DataFrame(grouped.get_group(1), columns=df.columns) # 1 = High income\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **5.1.2. Elbow Method** </span>\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.1.2.1 Low Income** </span>\n\n# %% [code]\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(2,15))\nvisualizer.fit(lowIncome)\nvisualizer.show()\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.1.2.1 High Income** </span>\n\n# %% [code]\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(2,15))\nvisualizer.fit(highIncome)\nvisualizer.show()\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **5.1.3. Silhoette Method** </span>\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.1.3.1 Low Income** </span>\n\n# %% [code]\nfor k in range(4,9):\n    #Instantiate the clustering model and visualizer\n    model = KMeans(k, random_state=1)\n    visualizer = SilhouetteVisualizer(model, colors='yellowbrick')\n    visualizer.fit(lowIncome)\n    visualizer.show()\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.1.3.2 High Income** </span>\n\n# %% [code]\nfor k in range(4,9):\n    #Instantiate the clustering model and visualizer\n    model = KMeans(k, random_state=1)\n    visualizer = SilhouetteVisualizer(model, colors='yellowbrick')\n    visualizer.fit(highIncome)\n    visualizer.show()\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **5.2. Implement Clustering Algorithms** </span>\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **5.2.1. KMeans Clustering Algorithm** </span>\n\n# %% [code]\nkmLow = KMeans(n_clusters=5)\nkmHigh = KMeans(n_clusters=6)\n\nKMeansClusters = kmLow.fit_predict(lowIncome)\nlowIncome['KMeansCluster'] = KMeansClusters\n\nKMeansClusters = kmHigh.fit_predict(highIncome)\nhighIncome['KMeansCluster'] = KMeansClusters\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **5.3. Analysis of Cluster Results** </span>\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **5.3.1. Years Coding Professionally** </span>\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.3.1.1 Low Income** </span>\n\n# %% [code]\nyearsCodePro_crosstab_low = pd.crosstab(lowIncome['KMeansCluster'],lowIncome[\"Years_bin\"],  normalize=True)\nyearsCodePro_crosstab_low.plot(kind = 'bar', grid=True).legend(loc='center left',bbox_to_anchor=(1.0, 0.5));\nplt.show()\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.3.1.2 High Income** </span>\n\n# %% [code]\nyearsCodePro_crosstab_high = pd.crosstab(highIncome['KMeansCluster'],highIncome[\"Years_bin\"],  normalize=True)\nyearsCodePro_crosstab_high.plot(kind = 'bar', grid=True).legend(loc='center left',bbox_to_anchor=(1.0, 0.5));\nplt.show()\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **5.3.2. Country** </span>\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.3.2.1 Low Income** </span>\n\n# %% [code]\ncountry_crosstab_low = pd.crosstab(lowIncome['KMeansCluster'], country[\"Country\"],  normalize=True)\ncountry_crosstab_low.plot(kind = 'bar', grid=True).legend(loc='center left',bbox_to_anchor=(1.0, 0.5));\nplt.show()\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.3.3.2 High Income** </span>\n\n# %% [code]\ncountry_crosstab_high = pd.crosstab(highIncome['KMeansCluster'], country[\"Country\"],  normalize=True)\ncountry_crosstab_high.plot(kind = 'bar', grid=True).legend(loc='center left',bbox_to_anchor=(1.0, 0.5));\nplt.show()\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **5.3.3. Education Level** </span>\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.3.3.1 Low Income** </span>\n\n# %% [code]\nedLevel_crosstab_low = pd.crosstab(lowIncome['KMeansCluster'],lowIncome[\"EdLevel\"],  normalize=True)\nedLevel_crosstab_low.plot(kind = 'bar', grid=True).legend(loc='center left',bbox_to_anchor=(1.0, 0.5)); \nplt.show()\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.3.3.2 High Income** </span>\n\n# %% [code]\nedLevel_crosstab_high = pd.crosstab(highIncome['KMeansCluster'],highIncome[\"EdLevel\"],  normalize=True)\nedLevel_crosstab_high.plot(kind = 'bar', grid=True).legend(loc='center left',bbox_to_anchor=(1.0, 0.5)); \nplt.show()\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **5.3.4. Developer Type** </span>\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.3.4.1 Low Income** </span>\n\n# %% [code]\ndevType_crosstab_low = pd.crosstab(lowIncome['KMeansCluster'], devSalary[\"DeveloperType\"],  normalize=True)\ndevType_crosstab_low.plot(kind = 'bar', grid=True).legend(loc='center left',bbox_to_anchor=(1.0, 0.5)); \nplt.show()\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.3.4.2 High Income** </span>\n\n# %% [code]\ndevType_crosstab_high = pd.crosstab(highIncome['KMeansCluster'], devSalary[\"DeveloperType\"],  normalize=True)\ndevType_crosstab_high.plot(kind = 'bar', grid=True).legend(loc='center left',bbox_to_anchor=(1.0, 0.5)); \nplt.show()\n\n# %% [markdown]\n# <span style=\"font-size:17px;\"> **5.3.5. Languages Used** </span>\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.3.5.1 Low Income** </span>\n\n# %% [code]\nlang_crosstab_low = pd.crosstab(lowIncome['KMeansCluster'], langSalary[\"Language\"],  normalize=True)\nlang_crosstab_low.plot(kind = 'bar', grid=True).legend(loc='center left',bbox_to_anchor=(1.0, 0.5)); \nplt.show()\n\n# %% [markdown]\n# <span style=\"font-size:15px;\"> **5.3.5.2 High Income** </span>\n\n# %% [code]\nlang_crosstab_high = pd.crosstab(highIncome['KMeansCluster'], langSalary[\"Language\"],  normalize=True)\nlang_crosstab_high.plot(kind = 'bar', grid=True).legend(loc='center left',bbox_to_anchor=(1.0, 0.5)); \nplt.show()\n\n# %% [markdown]\n# ###### <span style=\"font-size:25px;\"> **6. Model Implementation** </span>\n\n# %% [code]\ncols = [\n    'Algorithm',\n    'Accuracy',\n    'AUC',\n    'FPR',\n    'TPR',\n    'Dataset'\n]\nscores = pd.DataFrame(columns=cols)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **6.1. K-Nearest Neighbour** </span>\n\n# %% [code]\nscaler = StandardScaler()  \nscaler.fit(X_train)\nX_trainS = scaler.transform(X_train) \nX_testS = scaler.transform(X_test)\n\n# %% [code]\nknn = KNeighborsClassifier()\nknn.fit(X_trainS, y_train)\n\ny_train_pred = knn.predict(X_trainS)\ny_test_pred = knn.predict(X_testS)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\ntdf = pd.DataFrame([[\"KNN\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"KNN\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(knn, X_testS, y_test, cmap=plt.cm.Blues)\n\n# %% [code]\nknn_hp = KNeighborsClassifier()\n\nparam_grid = {'weights':['uniform', 'distance'], \n              'metric':['euclidean', 'manhattan', 'minkowski'], \n              'n_neighbors':list(range(1,21))}\n\ngs = GridSearchCV(knn_hp, param_grid, scoring='accuracy', cv=10)\ngs = gs.fit(X_trainS,y_train)\n\nparameter_best = gs.best_estimator_\nprint(\"best model:\", parameter_best.get_params())\n\n# Fit the best model to the data. \nparameter_best.fit(X_trainS, y_train)\n\ny_train_pred = parameter_best.predict(X_trainS)\ny_test_pred = parameter_best.predict(X_testS)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"KNN_HP\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"KNN_HP\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(gs, X_testS, y_test, cmap=plt.cm.Blues)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **6.2. Decision Tree** </span>\n\n# %% [code]\ndt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\n\ny_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"DecisionTree\", train_score, auc_score_train, fp_train, tp_train, \"Train\"], \n                    [\"DecisionTree\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(dt, X_test, y_test, cmap=plt.cm.Blues)\n\n# %% [code]\n# create Decision Tree classifer object\ndt_hp = DecisionTreeClassifier() \n\nparam_grid = {'criterion':['gini', 'entropy'], \n              'splitter':['best', 'random',], \n              'max_depth':list(range(1,21))}\n\n\ngs = RandomizedSearchCV(dt_hp, param_grid, scoring='accuracy', cv=10)                          \ngs = gs.fit(X_train,y_train)\n\nparameter_best = gs.best_estimator_\nprint(\"best model:\", parameter_best.get_params())\n\nparameter_best.fit(X_train, y_train)\n\ny_train_pred = parameter_best.predict(X_train)\ny_test_pred = parameter_best.predict(X_test)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"DecisionTree_HP\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"DecisionTree_HP\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(gs, X_test, y_test, cmap=plt.cm.Blues)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **6.3. Logistic Regression** </span>\n\n# %% [code]\nlogr = LogisticRegression()\nlogr.fit(X_train,y_train)\n\ny_train_pred = logr.predict(X_train)\ny_test_pred = logr.predict(X_test)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"LogisticRegression\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"LogisticRegression\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(logr, X_test, y_test, cmap=plt.cm.Blues)\n\n# %% [code]\n# Create LogisticRegression object\nlogr_hp = LogisticRegression(max_iter = 2000)\n\nparam_grid = {'solver':['newton-cg', 'lbfgs',  'liblinear', 'sag', 'saga'],\n              'penalty':['l2'],\n              'C':[100, 10, 1.0, 0.1, 0.01]}\n\ngs = GridSearchCV(logr_hp, param_grid, scoring='accuracy', cv=10)\ngs = gs.fit(X_train,y_train)\n\nparameter_best = gs.best_estimator_\nprint(\"best model:\", parameter_best.get_params())\n\nparameter_best.fit(X_train, y_train)\n\ny_train_pred = parameter_best.predict(X_train)\ny_test_pred = parameter_best.predict(X_test)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"LogisticRegression_HP\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"LogisticRegression_HP\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(gs, X_test, y_test, cmap=plt.cm.Blues)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **6.4. Artificial Neural Network** </span>\n\n# %% [code]\nscaler = StandardScaler()  \nscaler.fit(X_train)\nX_trainS = scaler.transform(X_train) \nX_testS = scaler.transform(X_test)\n\n# %% [code]\nann = MLPClassifier()\nann.fit(X_trainS,y_train)\n\ny_train_pred = ann.predict(X_trainS)\ny_test_pred = ann.predict(X_testS)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"ANN\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"ANN\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(ann, X_testS, y_test, cmap=plt.cm.Blues)\n\n# %% [code]\nann_hp = MLPClassifier(random_state=1, max_iter=1000)\n\nparam_grid = {'hidden_layer_sizes':[(3), (3,2),(4)],\n              'activation':['tanh','relu'],\n              'solver':['sgd','adam'],\n              'alpha':[0.0001, 0.005],\n              'learning_rate':['constant', 'adaptive']}\n\ngs = RandomizedSearchCV(ann_hp, param_grid, scoring='accuracy', cv=10)\ngs = gs.fit(X_trainS,y_train)\n\nparameter_best = gs.best_estimator_\nprint(\"best model:\", parameter_best.get_params())\n\nparameter_best.fit(X_trainS, y_train)\n\ny_train_pred = parameter_best.predict(X_trainS)\ny_test_pred = parameter_best.predict(X_testS)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"ANN_HP\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"ANN_HP\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(gs, X_testS, y_test, cmap=plt.cm.Blues)\n\n# %% [markdown]\n# ###### <span style=\"font-size:25px;\"> **7. Ensemble Learning** </span>\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **7.1. Random Forest** </span>\n\n# %% [code]\ndt_rf = RandomForestClassifier()\ndt_rf.fit(X_train,y_train)\n\ny_train_pred = dt_rf.predict(X_train)\ny_test_pred = dt_rf.predict(X_test)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"RandomForest\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"RandomForest\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(dt_rf, X_test, y_test, cmap=plt.cm.Blues)\n\n# %% [code]\ndt_rf_hp = RandomForestClassifier(random_state=1)\n\nparam_grid = {'max_features': ['auto', 'sqrt'],\n              'max_depth':list(range(1,21)),\n              'n_estimators':[10, 50, 100, 500, 1000, 1500, 2000]}\n\n\ngs = RandomizedSearchCV(dt_rf_hp, param_grid, scoring='accuracy', cv=10)                          \ngs = gs.fit(X_train,y_train)\n\nparameter_best = gs.best_estimator_\nprint(\"best model:\", parameter_best.get_params())\n\nparameter_best.fit(X_train, y_train)\n\ny_train_pred = parameter_best.predict(X_train)\ny_test_pred = parameter_best.predict(X_test)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"RandomForest_HP\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"RandomForest_HP\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(gs, X_test, y_test, cmap=plt.cm.Blues)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **7.2. Voting** </span>\n\n# %% [code]\nmodel1 = MLPClassifier(max_iter=1000, hidden_layer_sizes=4, activation='relu', solver='sgd', alpha=0.005, learning_rate='constant', random_state =1) \nmodel2 = KNeighborsClassifier(metric='euclidean', n_neighbors=19, weights='distance') \nmodel3 = LogisticRegression(max_iter=2000, solver='newton-cg', penalty='l2', C=1.0, random_state =1)\n\n# %% [code]\nvote = VotingClassifier(estimators=[('dt1', model1), ('dt2', model2), ('lr', model3)])\nvote = vote.fit(X_train, y_train)\n\ny_train_pred = vote.predict(X_train)\ny_test_pred = vote.predict(X_test)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"Voting\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"Voting\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(vote, X_test, y_test, cmap=plt.cm.Blues)\n\n# %% [code]\nvote_hp = VotingClassifier(estimators=[('dt1', model1), ('dt2', model2), ('lr', model3)])\n\nparam_grid = {'voting': ['hard', 'soft']}\n\ngs = GridSearchCV(vote_hp, param_grid, scoring='accuracy', cv=10)                          \ngs = gs.fit(X_train,y_train)\n\ny_train_pred = vote.predict(X_train)\ny_test_pred = vote.predict(X_test)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"Voting_HP\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"Voting_HP\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\n# %% [code]\nplot_confusion_matrix(gs, X_test, y_test, cmap=plt.cm.Blues)\n\n# %% [markdown]\n# <span style=\"font-size:20px;\"> **7.3. AdaBoost** </span>\n\n# %% [code]\ndt2 = DecisionTreeClassifier(max_depth=17, splitter='random', criterion= 'gini', random_state=1)\nada = AdaBoostClassifier(base_estimator=dt2, n_estimators=501,  random_state=1)\n\nada = ada.fit(X_train, y_train)\n\ny_train_pred = ada.predict(X_train)\ny_test_pred = ada.predict(X_test)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"Adaboost_dt\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"Adaboost_dt\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(ada, X_test, y_test, cmap=plt.cm.Blues)\n\n# %% [code]\nlogr2 = LogisticRegression(C=10, solver= 'lbfgs', penalty='l2', max_iter=2000)\nada = AdaBoostClassifier(base_estimator=logr2, n_estimators=501,  random_state=1)\n\nada = ada.fit(X_train, y_train)\n\ny_train_pred = ada.predict(X_train)\ny_test_pred = ada.predict(X_test)\n\ntrain_score = accuracy_score(y_train, y_train_pred)\ntest_score = accuracy_score(y_test, y_test_pred)\n\nfp_test, tp_test, th_test = roc_curve(y_test, y_test_pred)\nauc_score_test = auc(fp_test,tp_test)\n\nfp_train, tp_train, th_train = roc_curve(y_train, y_train_pred)\nauc_score_train = auc(fp_train,tp_train)\n\ntdf = pd.DataFrame([[\"Adaboost_logr\", train_score, auc_score_train, fp_train, tp_train, \"Train\"],\n                    [\"Adaboost_logr\", test_score, auc_score_test, fp_test, tp_test, \"Test\"]], columns=cols)\nscores = scores.append(tdf)\n\nprint('train/test accuracies %.3f/%.3f' % (train_score, test_score))\n\nplot_confusion_matrix(ada, X_test, y_test, cmap=plt.cm.Blues)\n\n# %% [code]\nsns.barplot(x=\"Accuracy\", y=\"Algorithm\", hue=\"Dataset\", data=scores).legend(loc='center left',bbox_to_anchor=(1.0, 0.5));\nplt.show()\n\n# %% [code]\ngroup = scores.groupby([\"Dataset\"])\n\n# %% [code]\nfor index, row in group.get_group(\"Train\").iterrows():\n    plt.plot(row[\"FPR\"], row[\"TPR\"], alpha=0.7, label='ROC %s (AUC = %0.3f)'  % (row[\"Algorithm\"], row[\"AUC\"]))\n    \n\nplt.title('ROC Curve comparison (Train)')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')    \nplt.show()\n\n# %% [code]\nfor index, row in group.get_group(\"Test\").iterrows():\n    plt.plot(row[\"FPR\"], row[\"TPR\"], alpha=0.7, label='ROC %s (AUC = %0.3f)'  % (row[\"Algorithm\"], row[\"AUC\"]))\n    \n\nplt.title('ROC Curve comparison (Test)')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')    \nplt.show()","metadata":{"_uuid":"6d50f8c3-2d89-4284-aa52-734115e3262f","_cell_guid":"a9dfc0cc-fdeb-4f82-b998-bb7581c92c8c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}